---
title: "AI and Mental Health. (under development)" 
editor: visual
---

## Position paper -- Primum non nocere! Opportunities and challenges of integrating AI tools in contemporary mental health clinical practice.

In comparison to related fields like education, the field of Mental Health clinical practice has witnessed limited technological transformation. From clinicians of the late 19th century to contemporary practitioners, clinical activities have centered around language. However, for the first time, this situation might be on the verge of change.

The rapid development of artificial intelligence tools with the capacity to emulate human language holds the potential to revolutionize Mental Health clinical practice. At present, these tools may come across as deceptive, non-empathic oracles without agency. Nevertheless, their design, such as typing responses character by character, readily encourages the attribution of awareness, fostering the impression of genuine human interaction.

Forecasting the evolution of these tools poses a challenge, yet it can be asserted with certainty that these tools are here to stay.

Our focus is centered on comprehending the intricate relationship between AI tools and the field of Mental Health clinical practice. Through this endeavour, our primary objective is to identify the challenges and opportunities stemming from this interaction and to formulate a series of recommendations aimed at mitigating negative outcomes and maximizing prospects within this field.

This challenge involves various stakeholders at different levels. It is important to note that while the following list is not exhaustive, it represents a preliminary exploration encompassing:

\- The impact on training, including continued education and accreditation.

\- The regulation of clinical practice (e.g., implications for practitioners' code of conduct).

\- The regulation of clinical research practices (e.g., considerations pertaining to Institutional Review Boards).

\- Strategies for disseminating and engaging with relevant stakeholders, including efforts to enhance AI literacy within the context of mental health clinical practice.

\- Considerations regarding the effects on diversity and equality policies related to access to mental health support.



In light of these profound changes on the horizon, we call for a comprehensive approach that includes:

\- The development of specialized training programs, workshops, and certifications in AI-assisted Mental Health practice to ensure that clinicians are proficient in utilizing these tools.

\- The establishment of clear ethical guidelines and standards of practice, incorporating AI interactions and communication as integral components of practitioners' code of conduct.

\- Rigorous oversight and evaluation mechanisms for AI-driven research practices, ensuring that ethical considerations, privacy, and data security are upheld, with the guidance of Institutional Review Boards.

\- Active engagement with stakeholders, including policymakers, healthcare professionals, and AI developers, to facilitate the dissemination of AI literacy in the context of mental health clinical practice.

\- The implementation of policies and initiatives aimed at fostering diversity and equity, thereby promoting accessible mental health support for all, including underrepresented populations around the world.

As we stand at the beginning of a new era in Mental Health clinical practice, these concrete steps are essential in shaping a future where AI enhances, rather than diminishes, the quality and accessibility of mental health care.


### Comprehending the intricate relationship between AI tools and the field of Mental Health clinical practice ###

One way to articulate this relationship involves considering the different levels of contact between AI tools and service users (SUs) (Table 1).

On one end of the spectrum, we find entirely human interactions between health agents and service users. As we move along the spectrum, a third agent, the AI tool, becomes involved in this clinical relationship, eventually reaching the opposite end where the health human agent disappears, leaving only the AI tool and the service user.

The primary focus of this reflective exercise lies in decision-making. As described below, the transition spans from purely human decision-making to shared decision-making and, finally, to purely AI decision making.

A second approach to reflect on this relationship involves evaluating how various capacities (cognitive, affective, manual, and moral) are implemented across the spectrum described above.

To do this, we adopt both an anthropocentric position and what we term a deceptive position.

In the deceptive stance, having agency is not prerequisite for the AI tool. Here, the critical factor is the AI tool\'s ability to imitate and persuade the service user that it possesses all four capabilities.

Table here?

Co-pilot: At this level, all final decisions are human-based decisions. 
The role of AI tools is restricted to providing support to human clinical practice. 
In this clinical scenario, the service user (SU) has no direct contact with the AI tool but can be aware of its role in the treatment process. 
Examples of Co-pilot may include assessment routines, where collection, analysis, and reports based on SU data are provided for clinicians. 
We could also observe treatment recommendations based on clinicians' prompts, for example, suggesting treatment routines to tackle issues derived from combinations of clinical vignettes and assessment inputs. 
  
The risk for humans here would be low. 
  
Augmented: Here, the clinician and AI tools work in parallel, sharing decisions. 
The service user is informed from the outset about the existence of AI tools and the possibility of having direct interactions with AI without total supervision from the clinician. 
In this scenario, SU consent would be a requirement. 
The clinician will attempt to have close monitoring of these AI interventions. 
Clinicians should invest in patient education regarding the risks and benefits of interacting with AI tools. 
An example of this could be homework prescribed by the clinician, decided and guided by AI tools, that is carried out outside clinical hours. The clinician may request (or have access to) records of these interactions, with the possibility of tailoring prompts to refine future interactions. 
  
The level of risk is medium. 
  
  
Supervised: In this case, all clinical decisions are AI-based, supervised by humans. 
The SU interacts with AI without direct clinician involvement. 
The SU might receive training on risks and benefits, but AI tools are entirely autonomous when interacting with SUs. 
The clinician serves as an external supervisor (e.g., Gessel camera observation) where they can access samples of interactions between AI and SUs but no longer have the capability to intervene in real time. These assessments conducted by clinicians could be used to adjust global AI parameters (calibration) to improve the quality performance of AI tools with future SUs. 
  
The level of risk is high. 
  
Autonomous: This is the most extreme scenario where the clinician's role disappears. 
Training resources for SUs interacting with AI tools can be provided, but it is not a prerequisite for accessing these AI tools. There is not necessarily control over the implementation of practices and results (lack of audits or quality control). 
  
The level of risk is very high. 

